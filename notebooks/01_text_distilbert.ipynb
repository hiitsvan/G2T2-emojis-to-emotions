{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fb96f5",
   "metadata": {},
   "source": [
    "# 01 ‚Äî Text (DistilBERT) fine-tune\n",
    "Train a text classifier and export per-sample probabilities and embeddings into `cache/`.\n",
    "**TODO**: point `df` to your dataframe with columns `[sample_id, text, label]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87140f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wasd0\\AppData\\Local\\Temp\\ipykernel_29816\\732169981.py:36: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df_raw = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'cyberbullying_tweets.csv' from andrewmvd/cyberbullying-classification via kagglehub.load_dataset\n",
      "Data source: andrewmvd/cyberbullying-classification/cyberbullying_tweets.csv\n",
      "  sample_id                                               text  \\\n",
      "0  s0000000  in other words katandandre your food was crapi...   \n",
      "1  s0000001  why is aussietv so white mkr theblock imaceleb...   \n",
      "2  s0000002         a classy whore or more red velvet cupcakes   \n",
      "\n",
      "               label  \n",
      "0  not_cyberbullying  \n",
      "1  not_cyberbullying  \n",
      "2  not_cyberbullying  \n",
      "label\n",
      "religion               7995\n",
      "age                    7992\n",
      "ethnicity              7951\n",
      "gender                 7878\n",
      "not_cyberbullying      7847\n",
      "other_cyberbullying    6089\n",
      "Name: count, dtype: int64\n",
      "df shape: (45752, 3)\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import kagglehub  # pip install kagglehub[pandas-datasets]\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ModuleNotFoundError:\n",
    "    print(\"kagglehub not found; installing kagglehub[pandas-datasets]...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kagglehub[pandas-datasets]\"])\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "DATASET_ID = \"andrewmvd/cyberbullying-classification\"\n",
    "FILE_CANDIDATES = [\n",
    "    \"cyberbullying_tweets.csv\",\n",
    "    \"train.csv\",\n",
    "    \"TRAIN.csv\",\n",
    "]\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[^a-z\\s']\", \" \", text)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "df_raw = None\n",
    "source_name = None\n",
    "load_errors = []\n",
    "\n",
    "for file_path in FILE_CANDIDATES:\n",
    "    try:\n",
    "        df_raw = kagglehub.load_dataset(\n",
    "            KaggleDatasetAdapter.PANDAS,\n",
    "            DATASET_ID,\n",
    "            file_path,\n",
    "        )\n",
    "        source_name = f\"{DATASET_ID}/{file_path}\"\n",
    "        print(f\"Loaded '{file_path}' from {DATASET_ID} via kagglehub.load_dataset\")\n",
    "        break\n",
    "    except FileNotFoundError as exc:\n",
    "        load_errors.append((file_path, exc))\n",
    "    except Exception as exc:\n",
    "        load_errors.append((file_path, exc))\n",
    "\n",
    "if df_raw is None:\n",
    "    print(\"Falling back to dataset_download; load_dataset attempts failed:\")\n",
    "    for candidate, exc in load_errors:\n",
    "        print(f\"  candidate='{candidate}': {exc}\")\n",
    "    dataset_dir = kagglehub.dataset_download(DATASET_ID)\n",
    "    candidates = glob.glob(os.path.join(dataset_dir, \"**/*.csv\"), recursive=True)\n",
    "\n",
    "    csv_path = None\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            hdr = pd.read_csv(path, nrows=0).columns.str.lower().tolist()\n",
    "            if (\"tweet_text\" in hdr or \"text\" in hdr) and \"cyberbullying_type\" in hdr:\n",
    "                csv_path = path\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if csv_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not locate a CSV with tweet_text/text and cyberbullying_type in {DATASET_ID}.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Using fallback CSV: {csv_path}\")\n",
    "    source_name = csv_path\n",
    "    df_raw = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Data source: {source_name}\")\n",
    "\n",
    "# Normalize expected column names\n",
    "cols = {c.lower(): c for c in df_raw.columns}\n",
    "text_col = cols.get(\"tweet_text\", cols.get(\"text\"))\n",
    "label_col = cols.get(\"cyberbullying_type\")\n",
    "assert text_col and label_col, \"CSV must contain tweet_text (or text) and cyberbullying_type.\"\n",
    "\n",
    "# Basic clean ‚Üí clean_text\n",
    "df = (\n",
    "    df_raw\n",
    "    .dropna(subset=[text_col, label_col])\n",
    "    .drop_duplicates(subset=[text_col])\n",
    "    .copy()\n",
    ")\n",
    "df[\"clean_text\"] = df[text_col].astype(str).apply(clean_tweet)\n",
    "df = df[df[\"clean_text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# Final columns expected by the DistilBERT notebook\n",
    "df = df.rename(columns={label_col: \"label\"})\n",
    "df.insert(0, \"sample_id\", [f\"s{i:07d}\" for i in range(len(df))])  # stable IDs\n",
    "df = df[[\"sample_id\", \"clean_text\", \"label\"]]\n",
    "\n",
    "df = df.rename(columns={\"clean_text\": \"text\"})[[\"sample_id\", \"text\", \"label\"]]\n",
    "\n",
    "print(df.head(3))\n",
    "print(df.label.value_counts())\n",
    "print(\"df shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af5b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text dataframe to ..\\data\\phase1\\text\\cyberbullying_text.parquet with 45752 rows\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text_data_dir = Path(\"../data/phase1/text\")\n",
    "text_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "text_parquet_path = text_data_dir / \"cyberbullying_text.parquet\"\n",
    "\n",
    "df.to_parquet(text_parquet_path, index=False)\n",
    "print(f\"Saved cleaned text dataframe to {text_parquet_path} with {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333e6ba",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"Lightweight fallback using frozen DistilBERT embeddings + logistic regression.\"\"\"\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nfor pkg, import_name in [\n    (\"transformers\", \"transformers\"),\n    (\"scikit-learn\", \"sklearn\"),\n    (\"joblib\", \"joblib\"),\n]:\n    try:\n        __import__(import_name)\n    except ModuleNotFoundError:\n        print(f\"Installing missing dependency: {pkg}\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n\nimport joblib\nimport numpy as np\nimport torch\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModel, AutoTokenizer\n\nfast_artifact_dir = Path(\"../artifacts/phase1/text/distilbert_fast\")\nfast_artifact_dir.mkdir(parents=True, exist_ok=True)\n\nlabel2id = {label: idx for idx, label in enumerate(sorted(df.label.unique()))}\nid2label = {idx: label for label, idx in label2id.items()}\nlabels = df[\"label\"].map(label2id).to_numpy()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\nmodel.eval()\n\nbatch_size = 128\nembedding_batches = []\nfor start in range(0, len(df), batch_size):\n    end = start + batch_size\n    batch_text = df.iloc[start:end][\"text\"].tolist()\n    encoded = tokenizer(\n        batch_text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n    )\n    encoded = {k: v.to(device) for k, v in encoded.items()}\n    with torch.no_grad():\n        outputs = model(**encoded)\n    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n    embedding_batches.append(cls_embeddings)\n\nembeddings = np.vstack(embedding_batches)\nnp.save(fast_artifact_dir / \"embeddings.npy\", embeddings)\n\nindices = np.arange(len(df))\n# Reserve 10% for test, then split remaining into train/validation (~80/10/10 overall)\ntrain_val_idx, test_idx, train_val_labels, test_labels = train_test_split(\n    indices,\n    labels,\n    test_size=0.1,\n    stratify=labels,\n    random_state=42,\n)\ntrain_idx, val_idx, y_train, y_val = train_test_split(\n    train_val_idx,\n    train_val_labels,\n    test_size=0.1111111111,  # 0.1 / 0.9\n    stratify=train_val_labels,\n    random_state=42,\n)\n\nX_train = embeddings[train_idx]\nX_val = embeddings[val_idx]\nX_test = embeddings[test_idx]\n\ntrain_df = df.iloc[train_idx].reset_index(drop=True)\nval_df = df.iloc[val_idx].reset_index(drop=True)\ntest_df = df.iloc[test_idx].reset_index(drop=True)\ntrain_df.insert(0, \"row_index\", train_idx)\nval_df.insert(0, \"row_index\", val_idx)\ntest_df.insert(0, \"row_index\", test_idx)\n\nclf = LogisticRegression(\n    multi_class=\"multinomial\",\n    solver=\"lbfgs\",\n    max_iter=500,\n    class_weight='balanced',  # Address class imbalance (1.31:1 ratio)\n)\nclf.fit(X_train, y_train)\njoblib.dump(clf, fast_artifact_dir / \"logreg.pkl\")\n\nval_preds = clf.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_preds)\nprint(f\"Validation accuracy (fast fallback): {val_accuracy:.4f}\")\n\ny_test = clf.predict(X_test)\nprint(f\"Test predictions generated for {len(y_test)} samples (stored for cache export).\")\n\nprobs = clf.predict_proba(embeddings)\nnp.save(fast_artifact_dir / \"probabilities.npy\", probs)\n\ntorch.save({\"label2id\": label2id, \"id2label\": id2label}, fast_artifact_dir / \"label_maps.pt\")\n\nper_sample = df.copy().reset_index(drop=True)\nfor idx, label in id2label.items():\n    per_sample[f\"prob_{label}\"] = probs[:, idx]\n\nper_sample_path = fast_artifact_dir / \"per_sample_probs.parquet\"\nper_sample.to_parquet(per_sample_path, index=False)\n\nprint(f\"Saved fast fallback artifacts to {fast_artifact_dir}\")\nprint(\"Artifacts include embeddings.npy, probabilities.npy, logreg.pkl, per_sample_probs.parquet, label_maps.pt\")\nprint(\"You can skip the fine-tune cell if this baseline suffices.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e8ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wasd0\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "üìÅ Model will be saved to: c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\distilbert\n",
      "   (Absolute path: C:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\distilbert)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed2d6a5b67b4ebab92c3c63622291f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cb03f0bdc94e73ac94c7c8bdd28c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wasd0\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6864' max='6864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6864/6864 5:15:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.358383</td>\n",
       "      <td>0.852694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.362834</td>\n",
       "      <td>0.864824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.472780</td>\n",
       "      <td>0.864277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wasd0\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\wasd0\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=6864, training_loss=0.2711372054540194, metrics={'train_runtime': 18929.5453, 'train_samples_per_second': 5.801, 'train_steps_per_second': 0.363, 'total_flos': 3636588838749696.0, 'train_loss': 0.2711372054540194, 'epoch': 3.0})\n",
      "\n",
      "üíæ Saving model to c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\distilbert...\n",
      "\n",
      "‚úÖ Model saved successfully!\n",
      "\n",
      "üìã Saved files:\n",
      "   - checkpoint-2288 (0.0 MB)\n",
      "   - checkpoint-4576 (0.0 MB)\n",
      "   - checkpoint-6864 (0.0 MB)\n",
      "   - config.json (0.0 MB)\n",
      "   - label_maps.pt (0.0 MB)\n",
      "   - model.safetensors (255.4 MB)\n",
      "   - runs (0.0 MB)\n",
      "   - special_tokens_map.json (0.0 MB)\n",
      "   - tokenizer.json (0.7 MB)\n",
      "   - tokenizer_config.json (0.0 MB)\n",
      "   - training_args.bin (0.0 MB)\n",
      "   - vocab.txt (0.2 MB)\n",
      "\n",
      "‚ú® Fine-tuned DistilBERT model is ready at: c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\distilbert\n",
      "   You can now use this model for inference on new data!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "for pkg, import_name in [\n",
    "    (\"transformers\", \"transformers\"),\n",
    "    (\"datasets\", \"datasets\"),\n",
    "    (\"accelerate\", \"accelerate\"),\n",
    "]:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Installing missing dependency: {pkg}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainingArguments)\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Use absolute path from project root\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "artifact_dir = PROJECT_ROOT / \"artifacts\" / \"phase1\" / \"text\" / \"distilbert\"\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Model will be saved to: {artifact_dir}\")\n",
    "print(f\"   (Absolute path: {artifact_dir.resolve()})\")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(df.label.unique()))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "def encode_labels(frame):\n",
    "    mapped = frame.copy()\n",
    "    mapped[\"label_id\"] = mapped[\"label\"].map(label2id)\n",
    "    return mapped\n",
    "\n",
    "train_df = encode_labels(train_df)\n",
    "val_df = encode_labels(val_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label_id\"]], preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df[[\"text\", \"label_id\"]], preserve_index=False)\n",
    "train_dataset = train_dataset.map(tokenize_batch, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_batch, batched=True)\n",
    "train_dataset = train_dataset.rename_column(\"label_id\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label_id\", \"labels\")\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(artifact_dir),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "train_output = trainer.train()\n",
    "print(train_output)\n",
    "\n",
    "print(f\"\\nüíæ Saving model to {artifact_dir}...\")\n",
    "trainer.save_model(str(artifact_dir))\n",
    "tokenizer.save_pretrained(str(artifact_dir))\n",
    "torch.save({\"label2id\": label2id, \"id2label\": id2label}, artifact_dir / \"label_maps.pt\")\n",
    "\n",
    "# Verify model files were saved\n",
    "print(f\"\\n‚úÖ Model saved successfully!\")\n",
    "print(f\"\\nüìã Saved files:\")\n",
    "for file in sorted(artifact_dir.glob(\"*\")):\n",
    "    size_mb = file.stat().st_size / 1024 / 1024\n",
    "    print(f\"   - {file.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\n‚ú® Fine-tuned DistilBERT model is ready at: {artifact_dir}\")\n",
    "print(f\"   You can now use this model for inference on new data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7287c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for model at: c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\distilbert\n",
      "‚úÖ Found fine-tuned model!\n",
      "üìä Generating probabilities and embeddings for 45752 samples...\n",
      "‚úÖ Saved probability matrix to c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\probabilities.npy with shape (45752, 6)\n",
      "‚úÖ Saved CLS embeddings to c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\embeddings.npy with shape (45752, 768)\n",
      "‚úÖ Saved per-sample probabilities to c:\\Users\\wasd0\\OneDrive\\Documents\\SMU\\Y3S1\\DM\\Data-Mining-G2T2\\artifacts\\phase1\\text\\per_sample_probs.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Use absolute paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "base_dir = PROJECT_ROOT / \"artifacts\" / \"phase1\" / \"text\"\n",
    "distilbert_dir = base_dir / \"distilbert\"\n",
    "fast_dir = base_dir / \"distilbert_fast\"\n",
    "\n",
    "label_map_file = distilbert_dir / \"label_maps.pt\"\n",
    "model_available = distilbert_dir.exists() and label_map_file.exists()\n",
    "\n",
    "print(f\"üîç Checking for model at: {distilbert_dir}\")\n",
    "\n",
    "if model_available:\n",
    "    print(f\"‚úÖ Found fine-tuned model!\")\n",
    "    \n",
    "    label_maps = torch.load(label_map_file)\n",
    "    label2id = label_maps[\"label2id\"]\n",
    "    id2label = label_maps[\"id2label\"]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(distilbert_dir).to(device)\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(distilbert_dir)\n",
    "\n",
    "    print(f\"üìä Generating probabilities and embeddings for {len(df)} samples...\")\n",
    "\n",
    "    batch_size = 64\n",
    "    prob_batches = []\n",
    "    embed_batches = []\n",
    "\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_text = df.iloc[start:end][\"text\"].tolist()\n",
    "        encoded = tokenizer(\n",
    "            batch_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded, output_hidden_states=True)\n",
    "        prob_batches.append(outputs.logits.softmax(dim=-1).cpu().numpy())\n",
    "        embed_batches.append(outputs.hidden_states[-1][:, 0, :].cpu().numpy())\n",
    "\n",
    "    probs = np.vstack(prob_batches)\n",
    "    embeddings = np.vstack(embed_batches)\n",
    "\n",
    "    np.save(base_dir / \"probabilities.npy\", probs)\n",
    "    np.save(base_dir / \"embeddings.npy\", embeddings)\n",
    "\n",
    "    per_sample = df.copy().reset_index(drop=True)\n",
    "    for idx, label in id2label.items():\n",
    "        per_sample[f\"prob_{label}\"] = probs[:, idx]\n",
    "\n",
    "    per_sample_path = base_dir / \"per_sample_probs.parquet\"\n",
    "    per_sample.to_parquet(per_sample_path, index=False)\n",
    "\n",
    "    np.save(base_dir / \"distilbert_sentence_embeddings.npy\", embeddings)\n",
    "\n",
    "    print(f\"‚úÖ Saved probability matrix to {base_dir / 'probabilities.npy'} with shape {probs.shape}\")\n",
    "    print(f\"‚úÖ Saved CLS embeddings to {base_dir / 'embeddings.npy'} with shape {embeddings.shape}\")\n",
    "    print(f\"‚úÖ Saved per-sample probabilities to {per_sample_path}\")\n",
    "\n",
    "elif fast_dir.exists():\n",
    "    print(\"‚ö†Ô∏è  Fine-tuned artifacts not found; using fast fallback artifacts.\")\n",
    "    probs_path = fast_dir / \"probabilities.npy\"\n",
    "    embeddings_path = fast_dir / \"embeddings.npy\"\n",
    "    per_sample_path = fast_dir / \"per_sample_probs.parquet\"\n",
    "    label_maps = torch.load(fast_dir / \"label_maps.pt\")\n",
    "    label2id = label_maps[\"label2id\"]\n",
    "    id2label = label_maps[\"id2label\"]\n",
    "\n",
    "    if not probs_path.exists() or not embeddings_path.exists():\n",
    "        raise FileNotFoundError(\"Fallback artifacts incomplete. Run the fast baseline cell first.\")\n",
    "\n",
    "    probs = np.load(probs_path)\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    per_sample = pd.read_parquet(per_sample_path)\n",
    "\n",
    "    np.save(base_dir / \"probabilities.npy\", probs)\n",
    "    np.save(base_dir / \"embeddings.npy\", embeddings)\n",
    "    per_sample.to_parquet(base_dir / \"per_sample_probs.parquet\", index=False)\n",
    "\n",
    "    print(\"‚úÖ Copied fallback probabilities/embeddings into the standard artifact directory.\")\n",
    "    print(f\"   Probability matrix shape: {probs.shape}\")\n",
    "    print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"   Per-sample probabilities saved to {base_dir / 'per_sample_probs.parquet'}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Neither fine-tuned DistilBERT nor fast fallback artifacts are available.\\n\"\n",
    "        f\"Expected locations:\\n\"\n",
    "        f\"  - {distilbert_dir}\\n\"\n",
    "        f\"  - {fast_dir}\\n\"\n",
    "        f\"Run one of the previous cells first.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99ea526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split distribution:\n",
      "split\n",
      "train         36601\n",
      "test           4576\n",
      "validation     4575\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Saved 4576 rows to ..\\cache\\text_probs_test.parquet\n",
      "‚úÖ Saved embeddings with shape (4576, 768) to ..\\cache\\text_emb_test.npy\n",
      "‚úÖ Saved 36601 rows to ..\\cache\\text_probs_train.parquet\n",
      "‚úÖ Saved embeddings with shape (36601, 768) to ..\\cache\\text_emb_train.npy\n",
      "‚úÖ Saved 4575 rows to ..\\cache\\text_probs_validation.parquet\n",
      "‚úÖ Saved embeddings with shape (4575, 768) to ..\\cache\\text_emb_validation.npy\n",
      "\n",
      "üéâ All cache files saved to ..\\cache\n",
      "Available files:\n",
      "  - text_emb_test.npy\n",
      "  - text_emb_train.npy\n",
      "  - text_emb_validation.npy\n",
      "  - text_probs_test.parquet\n",
      "  - text_probs_train.parquet\n",
      "  - text_probs_validation.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cache_dir = Path(\"../cache\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'per_sample' not in globals():\n",
    "    raise RuntimeError(\"Run the inference cell first to populate per_sample outputs.\")\n",
    "\n",
    "# Ensure we have consistent train/val/test splits\n",
    "# Check if splits were already defined from fine-tuning, otherwise create them\n",
    "if 'train_df' not in globals() or 'val_df' not in globals():\n",
    "    print(\"Creating consistent train/val/test splits...\")\n",
    "    \n",
    "    # Use the same random state and strategy as the fast fallback\n",
    "    indices = np.arange(len(df))\n",
    "    labels = df[\"label\"].map(label2id).to_numpy()\n",
    "    \n",
    "    # Reserve 10% for test, then split remaining into train/validation (~80/10/10 overall)\n",
    "    train_val_idx, test_idx, train_val_labels, test_labels = train_test_split(\n",
    "        indices,\n",
    "        labels,\n",
    "        test_size=0.1,\n",
    "        stratify=labels,\n",
    "        random_state=42,\n",
    "    )\n",
    "    train_idx, val_idx, y_train, y_val = train_test_split(\n",
    "        train_val_idx,\n",
    "        train_val_labels,\n",
    "        test_size=0.1111111111,  # 0.1 / 0.9 ‚âà 10% of total\n",
    "        stratify=train_val_labels,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    # Create dataframes with consistent splits\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "    test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Split sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Create split lookup dictionary\n",
    "split_lookup = {sid: \"train\" for sid in train_df[\"sample_id\"]}\n",
    "split_lookup.update({sid: \"validation\" for sid in val_df[\"sample_id\"]})\n",
    "split_lookup.update({sid: \"test\" for sid in test_df[\"sample_id\"]})\n",
    "\n",
    "# Add split information to per_sample dataframe\n",
    "per_sample_enriched = per_sample.reset_index(drop=False).rename(columns={\"index\": \"row_index\"})\n",
    "per_sample_enriched[\"split\"] = per_sample_enriched[\"sample_id\"].map(split_lookup)\n",
    "\n",
    "# Handle any samples not in the lookup (shouldn't happen, but safety check)\n",
    "if per_sample_enriched[\"split\"].isna().any():\n",
    "    print(\"‚ö†Ô∏è  Warning: Some samples couldn't be mapped to splits. Assigning to 'test'.\")\n",
    "    per_sample_enriched[\"split\"] = per_sample_enriched[\"split\"].fillna(\"test\")\n",
    "\n",
    "print(\"Split distribution:\")\n",
    "print(per_sample_enriched[\"split\"].value_counts())\n",
    "\n",
    "# Save split-specific files to cache\n",
    "for split_name in sorted(per_sample_enriched[\"split\"].unique()):\n",
    "    subset = per_sample_enriched[per_sample_enriched[\"split\"] == split_name].copy()\n",
    "    if subset.empty:\n",
    "        print(f\"‚ö†Ô∏è  No samples found for split '{split_name}', skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Get row indices for embeddings\n",
    "    row_indices = subset.pop(\"row_index\").to_numpy()\n",
    "    emb_subset = embeddings[row_indices]\n",
    "    \n",
    "    # Save probability and embedding files\n",
    "    probs_path = cache_dir / f\"text_probs_{split_name}.parquet\"\n",
    "    emb_path = cache_dir / f\"text_emb_{split_name}.npy\"\n",
    "    \n",
    "    subset.to_parquet(probs_path, index=False)\n",
    "    np.save(emb_path, emb_subset)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(subset)} rows to {probs_path}\")\n",
    "    print(f\"‚úÖ Saved embeddings with shape {emb_subset.shape} to {emb_path}\")\n",
    "\n",
    "print(f\"\\nüéâ All cache files saved to {cache_dir}\")\n",
    "print(\"Available files:\")\n",
    "for cache_file in sorted(cache_dir.glob(\"text_*\")):\n",
    "    print(f\"  - {cache_file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}