{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b32ccf",
   "metadata": {},
   "source": [
    "# 05 â€” Fusion head\n",
    "Load cached artifacts from previous steps and train a late-fusion or hybrid-fusion head.\n",
    "**TODO**: verify file presence in `cache/` before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983032eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cache from: /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache\n",
      "[WARN] Missing cached modal features at /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache/audio_probs.parquet\n",
      "[WARN] Missing cached modal features at /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache/audio_probs.parquet\n",
      "[WARN] Missing cached modal features at /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache/audio_probs.parquet\n",
      "{'train_rows': 36601, 'val_rows': 4575, 'test_rows': 4576}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path('..').resolve()))  # <-- make utils importable\n",
    "from utils.paths import CACHE_DIR\n",
    "\n",
    "\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Reading cache from: {CACHE_DIR.resolve()}')\n",
    "\n",
    "_DEF_FILL = 0.0\n",
    "\n",
    "LEX_PROBS_PATH = CACHE_DIR / 'lex_probs.parquet'\n",
    "# LEX_PROBS_PATH = CACHE_DIR / 'lex_probs_lightgbm.parquet'  # Uncomment to use LightGBM export\n",
    "\n",
    "AUDIO_PROBS_PATH = Path('../artifacts/phase1/audio_cnn/per_sample_probs.parquet')\n",
    "if not AUDIO_PROBS_PATH.exists():\n",
    "    AUDIO_PROBS_PATH = CACHE_DIR / 'audio_probs.parquet'\n",
    "\n",
    "_MODAL_FILES = {\n",
    "    'face': CACHE_DIR / 'face_probs_resnet50_gpu.parquet',\n",
    "    'lex': LEX_PROBS_PATH,\n",
    "    'emotion': CACHE_DIR / 'emotion_probs.parquet',\n",
    "    'audio': AUDIO_PROBS_PATH,\n",
    "}\n",
    "\n",
    "_DROP_COLS = {'label', 'split', 'text', 'raw_text', 'file_path', 'path'}\n",
    "\n",
    "def _maybe_read(path: Path) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        return pd.read_parquet(path)\n",
    "    print(f\"[WARN] Missing cached modal features at {path}\")\n",
    "    return pd.DataFrame(columns=['sample_id'])\n",
    "\n",
    "\n",
    "def _prepare_modal(df: pd.DataFrame, prefix: str, sample_ids: pd.Index) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df[df['sample_id'].isin(sample_ids)].copy()\n",
    "    drop_cols = [c for c in _DROP_COLS if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    df = df.sort_values('sample_id').drop_duplicates(subset='sample_id', keep='last')\n",
    "    rename_map = {c: f\"{prefix}_{c}\" for c in df.columns if c != 'sample_id' and not c.startswith(f\"{prefix}_\")}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_feats(split: str = 'train'):\n",
    "    split = split.lower()\n",
    "    if split in {'val', 'validation'}:\n",
    "        split = 'validation'\n",
    "    text_path = CACHE_DIR / f\"text_probs_{split}.parquet\"\n",
    "    emb_path = CACHE_DIR / f\"text_emb_{split}.npy\"\n",
    "    if not text_path.exists():\n",
    "        if split == 'test':\n",
    "            print(f\"[WARN] Missing text probabilities for split='{split}' at {text_path}; skipping.\")\n",
    "            return None, None\n",
    "        raise FileNotFoundError(f\"Missing text probabilities for split='{split}' at {text_path}\")\n",
    "    if not emb_path.exists():\n",
    "        if split == 'test':\n",
    "            print(f\"[WARN] Missing text embeddings for split='{split}' at {emb_path}; skipping.\")\n",
    "            return None, None\n",
    "        raise FileNotFoundError(f\"Missing text embeddings for split='{split}' at {emb_path}\")\n",
    "\n",
    "    base = pd.read_parquet(text_path).copy()\n",
    "\n",
    "    labels = base.set_index('sample_id')['label'].copy()\n",
    "    drop_from_base = [c for c in ('split', 'text', 'raw_text') if c in base.columns]\n",
    "    if drop_from_base:\n",
    "        base = base.drop(columns=drop_from_base)\n",
    "    features = base.set_index('sample_id').drop(columns=['label'])\n",
    "    sample_ids = features.index\n",
    "\n",
    "    for prefix, path in _MODAL_FILES.items():\n",
    "        modal = _prepare_modal(_maybe_read(path), prefix, sample_ids)\n",
    "        if modal.empty:\n",
    "            continue\n",
    "        modal = modal.set_index('sample_id')\n",
    "        features = features.join(modal, how='left')\n",
    "\n",
    "    features = features.fillna(_DEF_FILL)\n",
    "    features['label'] = labels\n",
    "    features = features.reset_index().rename(columns={'index': 'sample_id'})\n",
    "\n",
    "    embeddings = np.load(emb_path)\n",
    "    if len(features) != len(embeddings):\n",
    "        raise ValueError(\n",
    "            f\"Embedding count ({len(embeddings)}) does not match dataframe rows ({len(features)}).\"\n",
    "        )\n",
    "\n",
    "    return features, embeddings\n",
    "\n",
    "\n",
    "train_feats, train_emb = load_feats('train')\n",
    "val_feats, val_emb = load_feats('validation')\n",
    "test_feats, test_emb = load_feats('test')\n",
    "\n",
    "if train_feats is None or val_feats is None:\n",
    "    raise RuntimeError('Training and validation splits are required to train the fusion model.')\n",
    "\n",
    "split_sizes = {\n",
    "    'train_rows': len(train_feats),\n",
    "    'val_rows': len(val_feats),\n",
    "}\n",
    "if test_feats is not None:\n",
    "    split_sizes['test_rows'] = len(test_feats)\n",
    "else:\n",
    "    print('[WARN] Test split artifacts not found; fusion evaluation will skip test metrics.')\n",
    "\n",
    "print(split_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3682f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] audio validation metrics not found at ../artifacts/phase1/audio_cnn/validation_metrics.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "audio_metrics_path = Path('../artifacts/phase1/audio_cnn/validation_metrics.json')\n",
    "if audio_metrics_path.exists():\n",
    "    with audio_metrics_path.open() as f:\n",
    "        audio_metrics = json.load(f)\n",
    "    print('Loaded audio validation metrics:')\n",
    "    print({key: audio_metrics[key] for key in ['average_val_accuracy', 'std_val_accuracy', 'average_clean_train_accuracy'] if key in audio_metrics})\n",
    "else:\n",
    "    print('[WARN] audio validation metrics not found at', audio_metrics_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca642f",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n    precision_recall_fscore_support,\n    f1_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\n\nBULLY_LABEL = 'not_cyberbullying'\nDECISION_THRESHOLD = 0.6  # Adjusted threshold (default was 0.5)\n\nnumeric_cols = train_feats.select_dtypes(include=[np.number]).columns.tolist()\nfeature_cols = [c for c in numeric_cols if c != 'label']\n\n\ndef build_matrix(df, emb):\n    modal = df[feature_cols].to_numpy(dtype='float32', copy=True)\n    if emb is not None:\n        emb = emb.astype('float32')\n        if emb.ndim == 1:\n            emb = emb[:, None]\n        modal = np.concatenate([modal, emb], axis=1)\n    return modal\n\n\nX_train = build_matrix(train_feats, train_emb)\nX_val = build_matrix(val_feats, val_emb)\nX_test = build_matrix(test_feats, test_emb) if test_feats is not None and test_emb is not None else None\n\ny_train = (train_feats['label'].astype(str) != BULLY_LABEL).to_numpy(dtype='int32')\ny_val = (val_feats['label'].astype(str) != BULLY_LABEL).to_numpy(dtype='int32')\ny_test = (test_feats['label'].astype(str) != BULLY_LABEL).to_numpy(dtype='int32') if test_feats is not None else None\n\n# Check class distribution\nprint(\"Class distribution in training set:\")\nunique, counts = np.unique(y_train, return_counts=True)\nfor cls, cnt in zip(unique, counts):\n    print(f\"  Class {cls} ({'non_bullying' if cls == 0 else 'bullying'}): {cnt} samples ({cnt/len(y_train)*100:.2f}%)\")\n\nscaler = StandardScaler(with_mean=False)\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test) if X_test is not None else None\n\nfusion_clf = LogisticRegression(max_iter=1000, class_weight='balanced')\nfusion_clf.fit(X_train_scaled, y_train)\n\n# Get probabilities\nval_prob = fusion_clf.predict_proba(X_val_scaled)[:, 1]\n# Apply custom threshold\nval_pred = (val_prob >= DECISION_THRESHOLD).astype('int32')\n\nprint('\\n' + '='*60)\nprint('VALIDATION METRICS (Logistic Regression Fusion)')\nprint(f'Decision Threshold: {DECISION_THRESHOLD}')\nprint('='*60)\nprint(classification_report(y_val, val_pred, target_names=['non_bullying', 'bullying']))\nprint('ROC-AUC (val):', roc_auc_score(y_val, val_prob))\nprint('\\nConfusion matrix (val):')\ncm_val = confusion_matrix(y_val, val_pred)\nprint(cm_val)\n\n# Calculate error rates\ntn_val, fp_val, fn_val, tp_val = cm_val.ravel()\nfpr_val = fp_val / (fp_val + tn_val)  # False Positive Rate\nfnr_val = fn_val / (fn_val + tp_val)  # False Negative Rate\nprint(f'\\nError Analysis:')\nprint(f'  False Positive Rate (innocent flagged): {fpr_val:.2%} ({fp_val}/{fp_val + tn_val})')\nprint(f'  False Negative Rate (bullying missed): {fnr_val:.2%} ({fn_val}/{fn_val + tp_val})')\n\n# Plot validation confusion matrix\nfig, ax = plt.subplots(1, 1, figsize=(6, 5))\nsns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', cbar=True,\n            xticklabels=['non_bullying', 'bullying'],\n            yticklabels=['non_bullying', 'bullying'], ax=ax)\nax.set_ylabel('True Label')\nax.set_xlabel('Predicted Label')\nax.set_title(f'Validation Confusion Matrix\\n(Logistic Regression Fusion, threshold={DECISION_THRESHOLD})')\nplt.tight_layout()\nplt.show()\n\n# Test set predictions with custom threshold\nif X_test_scaled is not None:\n    test_prob = fusion_clf.predict_proba(X_test_scaled)[:, 1]\n    test_pred = (test_prob >= DECISION_THRESHOLD).astype('int32')\n    \n    print('\\n' + '='*60)\n    print('TEST METRICS (Logistic Regression Fusion)')\n    print(f'Decision Threshold: {DECISION_THRESHOLD}')\n    print('='*60)\n    print(classification_report(y_test, test_pred, target_names=['non_bullying', 'bullying']))\n    print('ROC-AUC (test):', roc_auc_score(y_test, test_prob))\n    print('\\nConfusion matrix (test):')\n    cm_test = confusion_matrix(y_test, test_pred)\n    print(cm_test)\n    \n    # Calculate error rates\n    tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n    fpr_test = fp_test / (fp_test + tn_test)\n    fnr_test = fn_test / (fn_test + tp_test)\n    print(f'\\nError Analysis:')\n    print(f'  False Positive Rate (innocent flagged): {fpr_test:.2%} ({fp_test}/{fp_test + tn_test})')\n    print(f'  False Negative Rate (bullying missed): {fnr_test:.2%} ({fn_test}/{fn_test + tp_test})')\n    \n    # Plot test confusion matrix\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens', cbar=True,\n                xticklabels=['non_bullying', 'bullying'],\n                yticklabels=['non_bullying', 'bullying'], ax=ax)\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\n    ax.set_title(f'Test Confusion Matrix\\n(Logistic Regression Fusion, threshold={DECISION_THRESHOLD})')\n    plt.tight_layout()\n    plt.show()\nelse:\n    test_pred = None\n    test_prob = None\n    print('\\n[WARN] Test split unavailable; skipping test evaluation.')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac76cfb",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nfusion_artifact_dir = Path('../artifacts/phase1/fusion')\nfusion_artifact_dir.mkdir(parents=True, exist_ok=True)\n\njoblib.dump({'scaler': scaler, 'model': fusion_clf, 'feature_cols': feature_cols}, fusion_artifact_dir / 'fusion_logreg.joblib')\nprint(f\"Saved fusion model to {fusion_artifact_dir / 'fusion_logreg.joblib'}\")\n\n# Compute comprehensive metrics for validation\nprecision_val, recall_val, f1_val, support_val = precision_recall_fscore_support(\n    y_val, val_pred, average=None\n)\n\nval_results = {\n    'decision_threshold': DECISION_THRESHOLD,\n    'roc_auc': float(roc_auc_score(y_val, val_prob)),\n    'accuracy': float((val_pred == y_val).mean()),\n    'precision_non_bullying': float(precision_val[0]),\n    'recall_non_bullying': float(recall_val[0]),\n    'f1_non_bullying': float(f1_val[0]),\n    'support_non_bullying': int(support_val[0]),\n    'precision_bullying': float(precision_val[1]),\n    'recall_bullying': float(recall_val[1]),\n    'f1_bullying': float(f1_val[1]),\n    'support_bullying': int(support_val[1]),\n    'macro_f1': float(f1_val.mean()),\n    'weighted_f1': float(f1_score(y_val, val_pred, average='weighted')),\n    'confusion_matrix': cm_val.tolist(),\n    'false_positive_rate': float(fpr_val),\n    'false_negative_rate': float(fnr_val),\n}\n\nmetrics = {'validation': val_results}\n\nif test_pred is not None and test_prob is not None:\n    # Compute comprehensive metrics for test\n    precision_test, recall_test, f1_test, support_test = precision_recall_fscore_support(\n        y_test, test_pred, average=None\n    )\n    \n    test_results = {\n        'decision_threshold': DECISION_THRESHOLD,\n        'roc_auc': float(roc_auc_score(y_test, test_prob)),\n        'accuracy': float((test_pred == y_test).mean()),\n        'precision_non_bullying': float(precision_test[0]),\n        'recall_non_bullying': float(recall_test[0]),\n        'f1_non_bullying': float(f1_test[0]),\n        'support_non_bullying': int(support_test[0]),\n        'precision_bullying': float(precision_test[1]),\n        'recall_bullying': float(recall_test[1]),\n        'f1_bullying': float(f1_test[1]),\n        'support_bullying': int(support_test[1]),\n        'macro_f1': float(f1_test.mean()),\n        'weighted_f1': float(f1_score(y_test, test_pred, average='weighted')),\n        'confusion_matrix': cm_test.tolist(),\n        'false_positive_rate': float(fpr_test),\n        'false_negative_rate': float(fnr_test),\n    }\n    metrics['test'] = test_results\n\n(fusion_artifact_dir / 'metrics.json').write_text(json.dumps(metrics, indent=2))\n\nprint('\\n' + '='*60)\nprint('SAVED METRICS SUMMARY')\nprint('='*60)\nprint(f\"Decision Threshold: {DECISION_THRESHOLD}\")\nprint(f\"Primary metric (ROC-AUC): {val_results['roc_auc']:.4f}\")\nprint(f\"Macro F1 (validation): {val_results['macro_f1']:.4f}\")\nprint(f\"Weighted F1 (validation): {val_results['weighted_f1']:.4f}\")\nprint(f\"FPR (validation): {val_results['false_positive_rate']:.2%}\")\nprint(f\"FNR (validation): {val_results['false_negative_rate']:.2%}\")\nif 'test' in metrics:\n    print(f\"Macro F1 (test): {metrics['test']['macro_f1']:.4f}\")\n    print(f\"Weighted F1 (test): {metrics['test']['weighted_f1']:.4f}\")\n    print(f\"FPR (test): {metrics['test']['false_positive_rate']:.2%}\")\n    print(f\"FNR (test): {metrics['test']['false_negative_rate']:.2%}\")\n    print('\\nSaved validation and test metrics to metrics.json')\nelse:\n    print('\\nSaved validation metrics to metrics.json (test split unavailable)')\nprint(f\"Location: {fusion_artifact_dir / 'metrics.json'}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d64e1",
   "metadata": {},
   "outputs": [],
   "source": "# Modal attention fusion (TensorFlow)\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, roc_auc_score\nimport json\n\nATTN_DIM = 128\nNUM_HEADS = 4\nATTN_DROPOUT = 0.1\nDENSE_DROPOUT = 0.3\nDECISION_THRESHOLD = 0.6  # Adjusted threshold (default was 0.5)\n\nmodal_prefixes = {\n    'text_prob': lambda c: c.startswith('prob_'),\n    'face': lambda c: c.startswith('face_'),\n    'audio': lambda c: c.startswith('audio_'),\n    'lex': lambda c: c.startswith('lex_'),\n    'emotion': lambda c: c.startswith('emotion_'),\n}\n\ndef make_modal_arrays(df, emb):\n    arrays = {}\n    for name, matcher in modal_prefixes.items():\n        cols = [c for c in feature_cols if matcher(c)]\n        if cols:\n            arrays[name] = df[cols].to_numpy(dtype='float32')\n    residual_cols = [\n        c for c in feature_cols\n        if not any(matcher(c) for matcher in modal_prefixes.values())\n    ]\n    if residual_cols:\n        arrays['other'] = df[residual_cols].to_numpy(dtype='float32')\n    if emb is not None:\n        arrays['text_emb'] = emb.astype('float32')\n    return arrays\n\ntrain_modal = make_modal_arrays(train_feats, train_emb)\nval_modal = make_modal_arrays(val_feats, val_emb)\ntest_modal = make_modal_arrays(test_feats, test_emb) if test_feats is not None and test_emb is not None else None\n\nshared_modalities = [name for name in train_modal if name in val_modal and train_modal[name].shape[1] > 0]\nif test_modal is not None:\n    shared_modalities = [name for name in shared_modalities if name in test_modal]\n\ntrain_inputs = {f\"{name}_input\": train_modal[name] for name in shared_modalities}\nval_inputs = {f\"{name}_input\": val_modal[name] for name in shared_modalities}\nif test_modal is not None:\n    test_inputs = {f\"{name}_input\": test_modal[name] for name in shared_modalities}\nelse:\n    test_inputs = None\n\ninputs = []\nmodal_tokens = []\nfor name in shared_modalities:\n    input_layer = layers.Input(shape=(train_modal[name].shape[1],), name=f\"{name}_input\")\n    projected = layers.Dense(ATTN_DIM, activation='relu', name=f\"{name}_proj\")(input_layer)\n    token = layers.Reshape((1, ATTN_DIM), name=f\"{name}_token\")(projected)\n    inputs.append(input_layer)\n    modal_tokens.append(token)\n\nif not modal_tokens:\n    raise RuntimeError('No modality features available for attention fusion.')\n\nif len(modal_tokens) > 1:\n    modal_sequence = layers.Concatenate(axis=1, name='modal_sequence')(modal_tokens)\nelse:\n    modal_sequence = modal_tokens[0]\n\nattn_output = layers.MultiHeadAttention(\n    num_heads=NUM_HEADS,\n    key_dim=ATTN_DIM // NUM_HEADS,\n    dropout=ATTN_DROPOUT,\n    name='modal_attention',\n)(modal_sequence, modal_sequence)\nresidual = layers.Add(name='attn_residual')([modal_sequence, attn_output])\npooled = layers.GlobalAveragePooling1D(name='modal_pool')(residual)\nx = layers.Dropout(DENSE_DROPOUT, name='pre_dense_dropout')(pooled)\nx = layers.Dense(ATTN_DIM, activation='relu', name='fusion_dense')(x)\nx = layers.Dropout(DENSE_DROPOUT, name='post_dense_dropout')(x)\noutput = layers.Dense(1, activation='sigmoid', name='bullying_prob')(x)\n\nattention_model = keras.Model(inputs=inputs, outputs=output, name='modal_attention_fusion')\nattention_model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss='binary_crossentropy',\n    metrics=[keras.metrics.AUC(name='roc_auc'), 'accuracy'],\n)\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_map = {int(c): float(w) for c, w in zip(np.unique(y_train), class_weights)}\n\nprint(f\"\\nClass weights for imbalance handling: {class_weight_map}\")\n\ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor='val_roc_auc',\n        mode='max',\n        patience=5,\n        restore_best_weights=True,\n    )\n]\n\nhistory = attention_model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(val_inputs, y_val),\n    epochs=40,\n    batch_size=128,\n    class_weight=class_weight_map,\n    callbacks=callbacks,\n    verbose=2,\n)\n\nval_probs_attn = attention_model.predict(val_inputs, batch_size=256)\n# Apply custom threshold\nval_pred_attn = (val_probs_attn >= DECISION_THRESHOLD).astype('int32').ravel()\n\nprint('\\n' + '='*60)\nprint('VALIDATION METRICS (Modal Attention Fusion)')\nprint(f'Decision Threshold: {DECISION_THRESHOLD}')\nprint('='*60)\nprint(classification_report(y_val, val_pred_attn, target_names=['non_bullying', 'bullying']))\nprint('ROC-AUC (val):', roc_auc_score(y_val, val_probs_attn.ravel()))\n\ncm_val_attn = confusion_matrix(y_val, val_pred_attn)\nprint('\\nConfusion matrix (val):')\nprint(cm_val_attn)\n\n# Calculate error rates\ntn_val_attn, fp_val_attn, fn_val_attn, tp_val_attn = cm_val_attn.ravel()\nfpr_val_attn = fp_val_attn / (fp_val_attn + tn_val_attn)\nfnr_val_attn = fn_val_attn / (fn_val_attn + tp_val_attn)\nprint(f'\\nError Analysis:')\nprint(f'  False Positive Rate (innocent flagged): {fpr_val_attn:.2%} ({fp_val_attn}/{fp_val_attn + tn_val_attn})')\nprint(f'  False Negative Rate (bullying missed): {fnr_val_attn:.2%} ({fn_val_attn}/{fn_val_attn + tp_val_attn})')\n\n# Plot validation confusion matrix for attention model\nfig, ax = plt.subplots(1, 1, figsize=(6, 5))\nsns.heatmap(cm_val_attn, annot=True, fmt='d', cmap='Purples', cbar=True,\n            xticklabels=['non_bullying', 'bullying'],\n            yticklabels=['non_bullying', 'bullying'], ax=ax)\nax.set_ylabel('True Label')\nax.set_xlabel('Predicted Label')\nax.set_title(f'Validation Confusion Matrix\\n(Modal Attention Fusion, threshold={DECISION_THRESHOLD})')\nplt.tight_layout()\nplt.show()\n\nif test_inputs is not None:\n    test_probs_attn = attention_model.predict(test_inputs, batch_size=256)\n    # Apply custom threshold\n    test_pred_attn = (test_probs_attn >= DECISION_THRESHOLD).astype('int32').ravel()\n    \n    print('\\n' + '='*60)\n    print('TEST METRICS (Modal Attention Fusion)')\n    print(f'Decision Threshold: {DECISION_THRESHOLD}')\n    print('='*60)\n    print(classification_report(y_test, test_pred_attn, target_names=['non_bullying', 'bullying']))\n    print('ROC-AUC (test):', roc_auc_score(y_test, test_probs_attn.ravel()))\n    \n    cm_test_attn = confusion_matrix(y_test, test_pred_attn)\n    print('\\nConfusion matrix (test):')\n    print(cm_test_attn)\n    \n    # Calculate error rates\n    tn_test_attn, fp_test_attn, fn_test_attn, tp_test_attn = cm_test_attn.ravel()\n    fpr_test_attn = fp_test_attn / (fp_test_attn + tn_test_attn)\n    fnr_test_attn = fn_test_attn / (fn_test_attn + tp_test_attn)\n    print(f'\\nError Analysis:')\n    print(f'  False Positive Rate (innocent flagged): {fpr_test_attn:.2%} ({fp_test_attn}/{fp_test_attn + tn_test_attn})')\n    print(f'  False Negative Rate (bullying missed): {fnr_test_attn:.2%} ({fn_test_attn}/{fn_test_attn + tp_test_attn})')\n    \n    # Plot test confusion matrix for attention model\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    sns.heatmap(cm_test_attn, annot=True, fmt='d', cmap='Oranges', cbar=True,\n                xticklabels=['non_bullying', 'bullying'],\n                yticklabels=['non_bullying', 'bullying'], ax=ax)\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\n    ax.set_title(f'Test Confusion Matrix\\n(Modal Attention Fusion, threshold={DECISION_THRESHOLD})')\n    plt.tight_layout()\n    plt.show()\nelse:\n    test_probs_attn = None\n    test_pred_attn = None\n\nattention_dir = fusion_artifact_dir / 'modal_attention'\nattention_dir.mkdir(parents=True, exist_ok=True)\nattention_model.save(attention_dir / 'fusion_modal_attention.keras')\nnp.save(attention_dir / 'val_probabilities.npy', val_probs_attn.ravel())\nif test_probs_attn is not None:\n    np.save(attention_dir / 'test_probabilities.npy', test_probs_attn.ravel())\n\n# Compute comprehensive metrics for validation\nprecision_val_attn, recall_val_attn, f1_val_attn, support_val_attn = precision_recall_fscore_support(\n    y_val, val_pred_attn, average=None\n)\n\nmetrics_attn = {\n    'validation': {\n        'decision_threshold': DECISION_THRESHOLD,\n        'roc_auc': float(roc_auc_score(y_val, val_probs_attn.ravel())),\n        'accuracy': float((val_pred_attn == y_val).mean()),\n        'precision_non_bullying': float(precision_val_attn[0]),\n        'recall_non_bullying': float(recall_val_attn[0]),\n        'f1_non_bullying': float(f1_val_attn[0]),\n        'support_non_bullying': int(support_val_attn[0]),\n        'precision_bullying': float(precision_val_attn[1]),\n        'recall_bullying': float(recall_val_attn[1]),\n        'f1_bullying': float(f1_val_attn[1]),\n        'support_bullying': int(support_val_attn[1]),\n        'macro_f1': float(f1_val_attn.mean()),\n        'weighted_f1': float(f1_score(y_val, val_pred_attn, average='weighted')),\n        'confusion_matrix': cm_val_attn.tolist(),\n        'false_positive_rate': float(fpr_val_attn),\n        'false_negative_rate': float(fnr_val_attn),\n    }\n}\n\nif test_pred_attn is not None:\n    # Compute comprehensive metrics for test\n    precision_test_attn, recall_test_attn, f1_test_attn, support_test_attn = precision_recall_fscore_support(\n        y_test, test_pred_attn, average=None\n    )\n    \n    metrics_attn['test'] = {\n        'decision_threshold': DECISION_THRESHOLD,\n        'roc_auc': float(roc_auc_score(y_test, test_probs_attn.ravel())),\n        'accuracy': float((test_pred_attn == y_test).mean()),\n        'precision_non_bullying': float(precision_test_attn[0]),\n        'recall_non_bullying': float(recall_test_attn[0]),\n        'f1_non_bullying': float(f1_test_attn[0]),\n        'support_non_bullying': int(support_test_attn[0]),\n        'precision_bullying': float(precision_test_attn[1]),\n        'recall_bullying': float(recall_test_attn[1]),\n        'f1_bullying': float(f1_test_attn[1]),\n        'support_bullying': int(support_test_attn[1]),\n        'macro_f1': float(f1_test_attn.mean()),\n        'weighted_f1': float(f1_score(y_test, test_pred_attn, average='weighted')),\n        'confusion_matrix': cm_test_attn.tolist(),\n        'false_positive_rate': float(fpr_test_attn),\n        'false_negative_rate': float(fnr_test_attn),\n    }\n\n(attention_dir / 'metrics.json').write_text(json.dumps(metrics_attn, indent=2))\n\nprint('\\n' + '='*60)\nprint('SAVED METRICS SUMMARY (Modal Attention)')\nprint('='*60)\nprint(f\"Decision Threshold: {DECISION_THRESHOLD}\")\nprint(f\"Primary metric (ROC-AUC): {metrics_attn['validation']['roc_auc']:.4f}\")\nprint(f\"Macro F1 (validation): {metrics_attn['validation']['macro_f1']:.4f}\")\nprint(f\"Weighted F1 (validation): {metrics_attn['validation']['weighted_f1']:.4f}\")\nprint(f\"FPR (validation): {metrics_attn['validation']['false_positive_rate']:.2%}\")\nprint(f\"FNR (validation): {metrics_attn['validation']['false_negative_rate']:.2%}\")\nif 'test' in metrics_attn:\n    print(f\"Macro F1 (test): {metrics_attn['test']['macro_f1']:.4f}\")\n    print(f\"Weighted F1 (test): {metrics_attn['test']['weighted_f1']:.4f}\")\n    print(f\"FPR (test): {metrics_attn['test']['false_positive_rate']:.2%}\")\n    print(f\"FNR (test): {metrics_attn['test']['false_negative_rate']:.2%}\")\nprint(f\"\\nSaved modal attention model to {attention_dir / 'fusion_modal_attention.keras'}\")\nprint(f\"Saved comprehensive metrics to {attention_dir / 'metrics.json'}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}