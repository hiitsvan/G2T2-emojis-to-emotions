{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b32ccf",
   "metadata": {},
   "source": [
    "# 05 — Fusion head\n",
    "Load cached artifacts from previous steps and train a late-fusion or hybrid-fusion head.\n",
    "**TODO**: verify file presence in `cache/` before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983032eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cache from: /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache\n",
      "[WARN] Missing cached modal features at /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache/audio_probs.parquet\n",
      "[WARN] Missing cached modal features at /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache/audio_probs.parquet\n",
      "[WARN] Missing cached modal features at /Users/mixberries13/Desktop/is424/G2T2-emojis-to-emotions/cache/audio_probs.parquet\n",
      "{'train_rows': 36601, 'val_rows': 4575, 'test_rows': 4576}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path('..').resolve()))  # <-- make utils importable\n",
    "from utils.paths import CACHE_DIR\n",
    "\n",
    "\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Reading cache from: {CACHE_DIR.resolve()}')\n",
    "\n",
    "_DEF_FILL = 0.0\n",
    "\n",
    "LEX_PROBS_PATH = CACHE_DIR / 'lex_probs.parquet'\n",
    "# LEX_PROBS_PATH = CACHE_DIR / 'lex_probs_lightgbm.parquet'  # Uncomment to use LightGBM export\n",
    "\n",
    "AUDIO_PROBS_PATH = Path('../artifacts/phase1/audio_cnn/per_sample_probs.parquet')\n",
    "if not AUDIO_PROBS_PATH.exists():\n",
    "    AUDIO_PROBS_PATH = CACHE_DIR / 'audio_probs.parquet'\n",
    "\n",
    "_MODAL_FILES = {\n",
    "    'face': CACHE_DIR / 'face_probs_resnet50_gpu.parquet',\n",
    "    'lex': LEX_PROBS_PATH,\n",
    "    'emotion': CACHE_DIR / 'emotion_probs.parquet',\n",
    "    'audio': AUDIO_PROBS_PATH,\n",
    "}\n",
    "\n",
    "_DROP_COLS = {'label', 'split', 'text', 'raw_text', 'file_path', 'path'}\n",
    "\n",
    "def _maybe_read(path: Path) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        return pd.read_parquet(path)\n",
    "    print(f\"[WARN] Missing cached modal features at {path}\")\n",
    "    return pd.DataFrame(columns=['sample_id'])\n",
    "\n",
    "\n",
    "def _prepare_modal(df: pd.DataFrame, prefix: str, sample_ids: pd.Index) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df[df['sample_id'].isin(sample_ids)].copy()\n",
    "    drop_cols = [c for c in _DROP_COLS if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    df = df.sort_values('sample_id').drop_duplicates(subset='sample_id', keep='last')\n",
    "    rename_map = {c: f\"{prefix}_{c}\" for c in df.columns if c != 'sample_id' and not c.startswith(f\"{prefix}_\")}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_feats(split: str = 'train'):\n",
    "    split = split.lower()\n",
    "    if split in {'val', 'validation'}:\n",
    "        split = 'validation'\n",
    "    text_path = CACHE_DIR / f\"text_probs_{split}.parquet\"\n",
    "    emb_path = CACHE_DIR / f\"text_emb_{split}.npy\"\n",
    "    if not text_path.exists():\n",
    "        if split == 'test':\n",
    "            print(f\"[WARN] Missing text probabilities for split='{split}' at {text_path}; skipping.\")\n",
    "            return None, None\n",
    "        raise FileNotFoundError(f\"Missing text probabilities for split='{split}' at {text_path}\")\n",
    "    if not emb_path.exists():\n",
    "        if split == 'test':\n",
    "            print(f\"[WARN] Missing text embeddings for split='{split}' at {emb_path}; skipping.\")\n",
    "            return None, None\n",
    "        raise FileNotFoundError(f\"Missing text embeddings for split='{split}' at {emb_path}\")\n",
    "\n",
    "    base = pd.read_parquet(text_path).copy()\n",
    "\n",
    "    labels = base.set_index('sample_id')['label'].copy()\n",
    "    drop_from_base = [c for c in ('split', 'text', 'raw_text') if c in base.columns]\n",
    "    if drop_from_base:\n",
    "        base = base.drop(columns=drop_from_base)\n",
    "    features = base.set_index('sample_id').drop(columns=['label'])\n",
    "    sample_ids = features.index\n",
    "\n",
    "    for prefix, path in _MODAL_FILES.items():\n",
    "        modal = _prepare_modal(_maybe_read(path), prefix, sample_ids)\n",
    "        if modal.empty:\n",
    "            continue\n",
    "        modal = modal.set_index('sample_id')\n",
    "        features = features.join(modal, how='left')\n",
    "\n",
    "    features = features.fillna(_DEF_FILL)\n",
    "    features['label'] = labels\n",
    "    features = features.reset_index().rename(columns={'index': 'sample_id'})\n",
    "\n",
    "    embeddings = np.load(emb_path)\n",
    "    if len(features) != len(embeddings):\n",
    "        raise ValueError(\n",
    "            f\"Embedding count ({len(embeddings)}) does not match dataframe rows ({len(features)}).\"\n",
    "        )\n",
    "\n",
    "    return features, embeddings\n",
    "\n",
    "\n",
    "train_feats, train_emb = load_feats('train')\n",
    "val_feats, val_emb = load_feats('validation')\n",
    "test_feats, test_emb = load_feats('test')\n",
    "\n",
    "if train_feats is None or val_feats is None:\n",
    "    raise RuntimeError('Training and validation splits are required to train the fusion model.')\n",
    "\n",
    "split_sizes = {\n",
    "    'train_rows': len(train_feats),\n",
    "    'val_rows': len(val_feats),\n",
    "}\n",
    "if test_feats is not None:\n",
    "    split_sizes['test_rows'] = len(test_feats)\n",
    "else:\n",
    "    print('[WARN] Test split artifacts not found; fusion evaluation will skip test metrics.')\n",
    "\n",
    "print(split_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3682f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] audio validation metrics not found at ../artifacts/phase1/audio_cnn/validation_metrics.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "audio_metrics_path = Path('../artifacts/phase1/audio_cnn/validation_metrics.json')\n",
    "if audio_metrics_path.exists():\n",
    "    with audio_metrics_path.open() as f:\n",
    "        audio_metrics = json.load(f)\n",
    "    print('Loaded audio validation metrics:')\n",
    "    print({key: audio_metrics[key] for key in ['average_val_accuracy', 'std_val_accuracy', 'average_clean_train_accuracy'] if key in audio_metrics})\n",
    "else:\n",
    "    print('[WARN] audio validation metrics not found at', audio_metrics_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cca642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "non_bullying       0.62      0.75      0.68       784\n",
      "    bullying       0.95      0.90      0.92      3791\n",
      "\n",
      "    accuracy                           0.88      4575\n",
      "   macro avg       0.78      0.83      0.80      4575\n",
      "weighted avg       0.89      0.88      0.88      4575\n",
      "\n",
      "ROC-AUC (val): 0.9252879402882228\n",
      "Confusion matrix (val):\n",
      "[[ 586  198]\n",
      " [ 366 3425]]\n",
      "\n",
      "Test metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "non_bullying       0.60      0.74      0.66       785\n",
      "    bullying       0.94      0.90      0.92      3791\n",
      "\n",
      "    accuracy                           0.87      4576\n",
      "   macro avg       0.77      0.82      0.79      4576\n",
      "weighted avg       0.88      0.87      0.88      4576\n",
      "\n",
      "ROC-AUC (test): 0.9288077528575052\n",
      "Confusion matrix (test):\n",
      "[[ 579  206]\n",
      " [ 382 3409]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "BULLY_LABEL = 'not_cyberbullying'\n",
    "\n",
    "numeric_cols = train_feats.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c != 'label']\n",
    "\n",
    "\n",
    "def build_matrix(df, emb):\n",
    "    modal = df[feature_cols].to_numpy(dtype='float32', copy=True)\n",
    "    if emb is not None:\n",
    "        emb = emb.astype('float32')\n",
    "        if emb.ndim == 1:\n",
    "            emb = emb[:, None]\n",
    "        modal = np.concatenate([modal, emb], axis=1)\n",
    "    return modal\n",
    "\n",
    "\n",
    "X_train = build_matrix(train_feats, train_emb)\n",
    "X_val = build_matrix(val_feats, val_emb)\n",
    "X_test = build_matrix(test_feats, test_emb) if test_feats is not None and test_emb is not None else None\n",
    "\n",
    "y_train = (train_feats['label'].astype(str) != BULLY_LABEL).to_numpy(dtype='int32')\n",
    "y_val = (val_feats['label'].astype(str) != BULLY_LABEL).to_numpy(dtype='int32')\n",
    "y_test = (test_feats['label'].astype(str) != BULLY_LABEL).to_numpy(dtype='int32') if test_feats is not None else None\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test) if X_test is not None else None\n",
    "\n",
    "fusion_clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "fusion_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "val_pred = fusion_clf.predict(X_val_scaled)\n",
    "val_prob = fusion_clf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print('Validation metrics:')\n",
    "print(classification_report(y_val, val_pred, target_names=['non_bullying', 'bullying']))\n",
    "print('ROC-AUC (val):', roc_auc_score(y_val, val_prob))\n",
    "print('Confusion matrix (val):')\n",
    "print(confusion_matrix(y_val, val_pred))\n",
    "\n",
    "test_pred = fusion_clf.predict(X_test_scaled) if X_test_scaled is not None else None\n",
    "test_prob = fusion_clf.predict_proba(X_test_scaled)[:, 1] if X_test_scaled is not None else None\n",
    "\n",
    "if test_pred is not None:\n",
    "    print()\n",
    "    print('Test metrics:')\n",
    "    print(classification_report(y_test, test_pred, target_names=['non_bullying', 'bullying']))\n",
    "    print('ROC-AUC (test):', roc_auc_score(y_test, test_prob))\n",
    "    print('Confusion matrix (test):')\n",
    "    print(confusion_matrix(y_test, test_pred))\n",
    "else:\n",
    "    print()\n",
    "    print('[WARN] Test split unavailable; skipping test evaluation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac76cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fusion model to ../artifacts/phase1/fusion/fusion_logreg.joblib\n",
      "Saved validation and test metrics.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fusion_artifact_dir = Path('../artifacts/phase1/fusion')\n",
    "fusion_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump({'scaler': scaler, 'model': fusion_clf, 'feature_cols': feature_cols}, fusion_artifact_dir / 'fusion_logreg.joblib')\n",
    "print(f\"Saved fusion model to {fusion_artifact_dir / 'fusion_logreg.joblib'}\")\n",
    "\n",
    "val_results = {\n",
    "    'roc_auc': float(roc_auc_score(y_val, val_prob)),\n",
    "    'accuracy': float((val_pred == y_val).mean()),\n",
    "}\n",
    "metrics = {'validation': val_results}\n",
    "if test_pred is not None and test_prob is not None:\n",
    "    test_results = {\n",
    "        'roc_auc': float(roc_auc_score(y_test, test_prob)),\n",
    "        'accuracy': float((test_pred == y_test).mean()),\n",
    "    }\n",
    "    metrics['test'] = test_results\n",
    "(fusion_artifact_dir / 'metrics.json').write_text(json.dumps(metrics, indent=2) + '')\n",
    "if 'test' in metrics:\n",
    "    print('Saved validation and test metrics.')\n",
    "else:\n",
    "    print('Saved validation metrics (test split unavailable).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89d64e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 15:24:51.670625: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-11-16 15:24:51.670665: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-11-16 15:24:51.670668: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-11-16 15:24:51.670893: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-16 15:24:51.670910: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 15:24:53.057677: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 - 17s - 60ms/step - accuracy: 0.9258 - loss: 0.1950 - roc_auc: 0.9769 - val_accuracy: 0.8745 - val_loss: 0.3338 - val_roc_auc: 0.9290\n",
      "Epoch 2/40\n",
      "286/286 - 12s - 44ms/step - accuracy: 0.9349 - loss: 0.1665 - roc_auc: 0.9824 - val_accuracy: 0.8798 - val_loss: 0.3480 - val_roc_auc: 0.9332\n",
      "Epoch 3/40\n",
      "286/286 - 12s - 43ms/step - accuracy: 0.9387 - loss: 0.1568 - roc_auc: 0.9844 - val_accuracy: 0.8667 - val_loss: 0.3543 - val_roc_auc: 0.9348\n",
      "Epoch 4/40\n",
      "286/286 - 12s - 43ms/step - accuracy: 0.9406 - loss: 0.1513 - roc_auc: 0.9855 - val_accuracy: 0.8785 - val_loss: 0.3270 - val_roc_auc: 0.9350\n",
      "Epoch 5/40\n",
      "286/286 - 12s - 43ms/step - accuracy: 0.9399 - loss: 0.1476 - roc_auc: 0.9863 - val_accuracy: 0.8739 - val_loss: 0.3380 - val_roc_auc: 0.9354\n",
      "Epoch 6/40\n",
      "286/286 - 13s - 44ms/step - accuracy: 0.9430 - loss: 0.1426 - roc_auc: 0.9872 - val_accuracy: 0.8726 - val_loss: 0.3425 - val_roc_auc: 0.9351\n",
      "Epoch 7/40\n",
      "286/286 - 13s - 44ms/step - accuracy: 0.9433 - loss: 0.1449 - roc_auc: 0.9867 - val_accuracy: 0.8662 - val_loss: 0.3626 - val_roc_auc: 0.9347\n",
      "Epoch 8/40\n",
      "286/286 - 12s - 42ms/step - accuracy: 0.9419 - loss: 0.1459 - roc_auc: 0.9863 - val_accuracy: 0.8748 - val_loss: 0.3196 - val_roc_auc: 0.9324\n",
      "Epoch 9/40\n",
      "286/286 - 12s - 42ms/step - accuracy: 0.9399 - loss: 0.1492 - roc_auc: 0.9855 - val_accuracy: 0.8702 - val_loss: 0.3864 - val_roc_auc: 0.9308\n",
      "Epoch 10/40\n",
      "286/286 - 12s - 41ms/step - accuracy: 0.9376 - loss: 0.1598 - roc_auc: 0.9842 - val_accuracy: 0.8776 - val_loss: 0.3197 - val_roc_auc: 0.9353\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Validation metrics (modal attention):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "non_bullying       0.60      0.78      0.68       784\n",
      "    bullying       0.95      0.89      0.92      3791\n",
      "\n",
      "    accuracy                           0.87      4575\n",
      "   macro avg       0.78      0.84      0.80      4575\n",
      "weighted avg       0.89      0.87      0.88      4575\n",
      "\n",
      "ROC-AUC (val): 0.9360912526445557\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "Test metrics (modal attention):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "non_bullying       0.60      0.80      0.69       785\n",
      "    bullying       0.96      0.89      0.92      3791\n",
      "\n",
      "    accuracy                           0.87      4576\n",
      "   macro avg       0.78      0.85      0.80      4576\n",
      "weighted avg       0.89      0.87      0.88      4576\n",
      "\n",
      "ROC-AUC (test): 0.9413451570682829\n",
      "Saved modal attention model to ../artifacts/phase1/fusion/modal_attention/fusion_modal_attention.keras\n"
     ]
    }
   ],
   "source": [
    "# Modal attention fusion (TensorFlow)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import json\n",
    "\n",
    "ATTN_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "ATTN_DROPOUT = 0.1\n",
    "DENSE_DROPOUT = 0.3\n",
    "\n",
    "modal_prefixes = {\n",
    "    'text_prob': lambda c: c.startswith('prob_'),\n",
    "    'face': lambda c: c.startswith('face_'),\n",
    "    'audio': lambda c: c.startswith('audio_'),\n",
    "    'lex': lambda c: c.startswith('lex_'),\n",
    "    'emotion': lambda c: c.startswith('emotion_'),\n",
    "}\n",
    "\n",
    "def make_modal_arrays(df, emb):\n",
    "    arrays = {}\n",
    "    for name, matcher in modal_prefixes.items():\n",
    "        cols = [c for c in feature_cols if matcher(c)]\n",
    "        if cols:\n",
    "            arrays[name] = df[cols].to_numpy(dtype='float32')\n",
    "    residual_cols = [\n",
    "        c for c in feature_cols\n",
    "        if not any(matcher(c) for matcher in modal_prefixes.values())\n",
    "    ]\n",
    "    if residual_cols:\n",
    "        arrays['other'] = df[residual_cols].to_numpy(dtype='float32')\n",
    "    if emb is not None:\n",
    "        arrays['text_emb'] = emb.astype('float32')\n",
    "    return arrays\n",
    "\n",
    "train_modal = make_modal_arrays(train_feats, train_emb)\n",
    "val_modal = make_modal_arrays(val_feats, val_emb)\n",
    "test_modal = make_modal_arrays(test_feats, test_emb) if test_feats is not None and test_emb is not None else None\n",
    "\n",
    "shared_modalities = [name for name in train_modal if name in val_modal and train_modal[name].shape[1] > 0]\n",
    "if test_modal is not None:\n",
    "    shared_modalities = [name for name in shared_modalities if name in test_modal]\n",
    "\n",
    "train_inputs = {f\"{name}_input\": train_modal[name] for name in shared_modalities}\n",
    "val_inputs = {f\"{name}_input\": val_modal[name] for name in shared_modalities}\n",
    "if test_modal is not None:\n",
    "    test_inputs = {f\"{name}_input\": test_modal[name] for name in shared_modalities}\n",
    "else:\n",
    "    test_inputs = None\n",
    "\n",
    "inputs = []\n",
    "modal_tokens = []\n",
    "for name in shared_modalities:\n",
    "    input_layer = layers.Input(shape=(train_modal[name].shape[1],), name=f\"{name}_input\")\n",
    "    projected = layers.Dense(ATTN_DIM, activation='relu', name=f\"{name}_proj\")(input_layer)\n",
    "    token = layers.Reshape((1, ATTN_DIM), name=f\"{name}_token\")(projected)\n",
    "    inputs.append(input_layer)\n",
    "    modal_tokens.append(token)\n",
    "\n",
    "if not modal_tokens:\n",
    "    raise RuntimeError('No modality features available for attention fusion.')\n",
    "\n",
    "if len(modal_tokens) > 1:\n",
    "    modal_sequence = layers.Concatenate(axis=1, name='modal_sequence')(modal_tokens)\n",
    "else:\n",
    "    modal_sequence = modal_tokens[0]\n",
    "\n",
    "attn_output = layers.MultiHeadAttention(\n",
    "    num_heads=NUM_HEADS,\n",
    "    key_dim=ATTN_DIM // NUM_HEADS,\n",
    "    dropout=ATTN_DROPOUT,\n",
    "    name='modal_attention',\n",
    ")(modal_sequence, modal_sequence)\n",
    "residual = layers.Add(name='attn_residual')([modal_sequence, attn_output])\n",
    "pooled = layers.GlobalAveragePooling1D(name='modal_pool')(residual)\n",
    "x = layers.Dropout(DENSE_DROPOUT, name='pre_dense_dropout')(pooled)\n",
    "x = layers.Dense(ATTN_DIM, activation='relu', name='fusion_dense')(x)\n",
    "x = layers.Dropout(DENSE_DROPOUT, name='post_dense_dropout')(x)\n",
    "output = layers.Dense(1, activation='sigmoid', name='bullying_prob')(x)\n",
    "\n",
    "attention_model = keras.Model(inputs=inputs, outputs=output, name='modal_attention_fusion')\n",
    "attention_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[keras.metrics.AUC(name='roc_auc'), 'accuracy'],\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_map = {int(c): float(w) for c, w in zip(np.unique(y_train), class_weights)}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_roc_auc',\n",
    "        mode='max',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "history = attention_model.fit(\n",
    "    train_inputs,\n",
    "    y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    class_weight=class_weight_map,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "val_probs_attn = attention_model.predict(val_inputs, batch_size=256)\n",
    "val_pred_attn = (val_probs_attn >= 0.5).astype('int32')\n",
    "print('Validation metrics (modal attention):')\n",
    "print(classification_report(y_val, val_pred_attn.ravel(), target_names=['non_bullying', 'bullying']))\n",
    "print('ROC-AUC (val):', roc_auc_score(y_val, val_probs_attn.ravel()))\n",
    "\n",
    "if test_inputs is not None:\n",
    "    test_probs_attn = attention_model.predict(test_inputs, batch_size=256)\n",
    "    test_pred_attn = (test_probs_attn >= 0.5).astype('int32')\n",
    "    print('\\nTest metrics (modal attention):')\n",
    "    print(classification_report(y_test, test_pred_attn.ravel(), target_names=['non_bullying', 'bullying']))\n",
    "    print('ROC-AUC (test):', roc_auc_score(y_test, test_probs_attn.ravel()))\n",
    "else:\n",
    "    test_probs_attn = None\n",
    "\n",
    "attention_dir = fusion_artifact_dir / 'modal_attention'\n",
    "attention_dir.mkdir(parents=True, exist_ok=True)\n",
    "attention_model.save(attention_dir / 'fusion_modal_attention.keras')\n",
    "np.save(attention_dir / 'val_probabilities.npy', val_probs_attn.ravel())\n",
    "if test_probs_attn is not None:\n",
    "    np.save(attention_dir / 'test_probabilities.npy', test_probs_attn.ravel())\n",
    "\n",
    "metrics = {\n",
    "    'validation': {\n",
    "        'roc_auc': float(roc_auc_score(y_val, val_probs_attn.ravel())),\n",
    "        'accuracy': float((val_pred_attn.ravel() == y_val).mean()),\n",
    "    }\n",
    "}\n",
    "if test_probs_attn is not None:\n",
    "    metrics['test'] = {\n",
    "        'roc_auc': float(roc_auc_score(y_test, test_probs_attn.ravel())),\n",
    "        'accuracy': float((test_pred_attn.ravel() == y_test).mean()),\n",
    "    }\n",
    "(attention_dir / 'metrics.json').write_text(json.dumps(metrics, indent=2) + '')\n",
    "print(f\"Saved modal attention model to {attention_dir / 'fusion_modal_attention.keras'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
